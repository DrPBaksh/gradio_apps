{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0abfe11-6ff8-4646-bd2a-e45eb541b3bb",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DrPBaksh/image_processing/blob/main/logos/logo_2.png?raw=true\" alt=\"Corndel\" width =\"301.5\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d012f-a8c9-4b06-8387-42df49cb79d1",
   "metadata": {},
   "source": [
    "# Image Classifiers using Gradio and Hugging Face\n",
    "\n",
    "1. Import necessary libraries including gradio, torch, clip, and PIL.\n",
    "2. Load the CLIP model and specify the device (CPU or GPU) to use. Pre trained image classifier\n",
    "3. Define a function classify_image that takes an input image and a list of categories, preprocesses the image, encodes the image and text using the 4 4. CLIP model, and returns a list of labels and scores as results.\n",
    "5. Define the inputs and outputs for the Gradio interface using the gr.inputs and gr.outputs functions.\n",
    "6. Define the title, description, and examples for the Gradio interface.\n",
    "7. Download example images from GitHub using the requests library and save them locally.\n",
    "8. Launch the Gradio interface using the gr.Interface function and the launch method, specifying that the interface should be shareable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb72ed08-789f-48d9-b8ea-b6d9133f4067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install openai_clip\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e534a8f-a2db-466c-8cca-60cc323ea4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import requests\n",
    "\n",
    "\n",
    "# Load the CLIP model and preprocess\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Get example images\n",
    "\n",
    "urls = ['https://github.com/DrPBaksh/workshop-data/raw/main/parrot.jpg', 'https://github.com/DrPBaksh/workshop-data/raw/main/cat.jpg']\n",
    "print('downloading examples')\n",
    "for i in range(len(urls)):\n",
    "    url = urls[i]\n",
    "    response = requests.get(url)\n",
    "    with open(urls[i].split('/')[-1], 'wb') as f:\n",
    "        f.write(response.content)\n",
    "print('finished downloading examples')\n",
    "\n",
    "\n",
    "# Define the Gradio interface\n",
    "def classify_image(input_image, categories):\n",
    "    # Prepare the image\n",
    "    input_image = Image.fromarray(np.uint8(input_image))\n",
    "    input_image = input_image.convert(\"RGB\")\n",
    "    input_image = preprocess(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare the candidate labels\n",
    "    candidate_labels = categories.split(\", \")\n",
    "    text = clip.tokenize(candidate_labels).to(device)\n",
    "\n",
    "    # Make a prediction using the classifier pipeline\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(input_image)\n",
    "        text_features = model.encode_text(text)\n",
    "        logits_per_image = torch.matmul(image_features, text_features.t())\n",
    "        probs = logits_per_image.softmax(dim=-1)\n",
    "\n",
    "    # Format the prediction results as a list of labels and scores\n",
    "    results = []\n",
    "    for i, (label, prob) in enumerate(zip(candidate_labels, probs[0])):\n",
    "        results.append(\"{}: {:.2f}%\".format(label, prob.item() * 100))\n",
    "\n",
    "    # Return the formatted results\n",
    "    return results\n",
    "\n",
    "inputs = [\n",
    "    gr.inputs.Image(label=\"Input Image\"),\n",
    "    gr.inputs.Textbox(label=\"Categories\", default=\"dog, cat, bird\")\n",
    "]\n",
    "outputs = gr.outputs.Textbox(label=\"Image Classification Results\")\n",
    "title = \"Zero-Shot Image Classifier\"\n",
    "description = \"Upload an image and select the categories you want to test the image against using a zero-shot learning model.\"\n",
    "examples = [\n",
    "    [\"parrot.jpg\", \"bird\"],\n",
    "    [\"cat.jpg\", \"dog, cat\"]\n",
    "]\n",
    "\n",
    "gr.Interface(fn=classify_image, inputs=inputs, outputs=outputs, title=title, description=description, examples=examples).launch(share = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eee025-7fc7-4db4-93cc-971c99f82730",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Image Classifiers using Gradio and Hugging Face\n",
    "\n",
    "The code below is an image classifier where you do not need to enter in the categoires. It uses the already divided categories from imagenet to categorise your image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "513b268a-9cd7-4146-82ad-c6f819f5c9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading examples\n",
      "finished downloading examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pete/Documents/python_code/hugginFace/venv_hugginFace/lib/python3.8/site-packages/gradio/inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/pete/Documents/python_code/hugginFace/venv_hugginFace/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/pete/Documents/python_code/hugginFace/venv_hugginFace/lib/python3.8/site-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the CLIP model and preprocess\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "urls = ['https://github.com/DrPBaksh/workshop-data/raw/main/parrot.jpg', 'https://github.com/DrPBaksh/workshop-data/raw/main/cat.jpg']\n",
    "print('downloading examples')\n",
    "for i in range(len(urls)):\n",
    "    url = urls[i]\n",
    "    response = requests.get(url)\n",
    "    with open(urls[i].split('/')[-1], 'wb') as f:\n",
    "        f.write(response.content)\n",
    "print('finished downloading examples')\n",
    "\n",
    "\n",
    "\n",
    "# Get all 1000 ImageNet categories\n",
    "with open(\"imagenet_Classes.txt\") as f:\n",
    "    candidate_labels = f.readlines()\n",
    "candidate_labels = [label.strip() for label in candidate_labels]\n",
    "\n",
    "# Define the Gradio interface\n",
    "def classify_image(input_image):\n",
    "    # Prepare the image\n",
    "    input_image = Image.fromarray(np.uint8(input_image))\n",
    "    input_image = input_image.convert(\"RGB\")\n",
    "    input_image = preprocess(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare the candidate labels\n",
    "    text = clip.tokenize(candidate_labels).to(device)\n",
    "\n",
    "    # Make a prediction using the classifier pipeline\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(input_image)\n",
    "        text_features = model.encode_text(text)\n",
    "        logits_per_image = torch.matmul(image_features, text_features.t())\n",
    "        probs = logits_per_image.softmax(dim=-1)\n",
    "\n",
    "    # Get the top 3 labels and probabilities\n",
    "    top_k = torch.topk(probs, k=3)\n",
    "    top_probs = top_k.values[0]\n",
    "    top_labels = [candidate_labels[i] for i in top_k.indices[0]]\n",
    "\n",
    "    # Format the prediction results as a list of labels and scores\n",
    "    results = []\n",
    "    for i in range(len(top_labels)):\n",
    "        results.append(\"{}: {:.2f}%\".format(top_labels[i], top_probs[i].item() * 100))\n",
    "\n",
    "    # Return the formatted results\n",
    "    return results\n",
    "\n",
    "\n",
    "inputs = gr.inputs.Image(label=\"Input Image\")\n",
    "outputs = gr.outputs.Textbox(label=\"Image Classification Results\")\n",
    "title = \"Zero-Shot Image Classifier\"\n",
    "description = \"Upload an image and we'll predict its category using a zero-shot learning model trained on the ImageNet dataset.\"\n",
    "examples = [[\"cat.jpg\", \"parrot.jpg\"]]\n",
    "\n",
    "gr.Interface(fn=classify_image, inputs=inputs, outputs=outputs, title=title, description=description, examples=examples).launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4021b-7010-4f40-8176-e5d3e6aab2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
